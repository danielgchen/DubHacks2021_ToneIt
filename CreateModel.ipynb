{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DubHacks2021_ToneIt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFAudkAYZr6M"
      },
      "source": [
        "#Download the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fBCoV7JYREQF",
        "outputId": "be1cb0be-4365-47ee-a0a2-9695e508a0b0"
      },
      "source": [
        "import os\n",
        "# > Old Package Versions\n",
        "# # Install java\n",
        "# ! apt-get update -qq\n",
        "# ! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "# ! java -version\n",
        "# # Install pyspark\n",
        "# ! pip install --ignore-installed pyspark==2.4.4\n",
        "# # Install Spark NLP\n",
        "# ! pip install --ignore-installed spark-nlp==2.5.1\n",
        "\n",
        "# > New Package Versions\n",
        "! pip install -q pyspark==3.1.2 spark-nlp\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.4 MB 57 kB/s \n",
            "\u001b[K     |████████████████████████████████| 122 kB 53.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 56.6 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08JBKhsdZqF_"
      },
      "source": [
        "# Train the model\n",
        "## Import the pacakges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "q137DmdURQU8",
        "outputId": "1cb6933d-8496-4d99-f402-bd1ed8447d4c"
      },
      "source": [
        "import sparknlp\n",
        "spark = sparknlp.start(gpu = True) # for GPU training >> sparknlp.start(gpu = True) # for Spark 2.3 =>> sparknlp.start(spark23 = True)\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Spark NLP version\", sparknlp.version())\n",
        "print(\"Apache Spark version:\", spark.version)\n",
        "\n",
        "spark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version 3.3.1\n",
            "Apache Spark version: 3.1.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b390b9615062:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7efd33418d10>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0ypzzy1Z6P3"
      },
      "source": [
        "##Get the training dataset\n",
        "###Clean the dataset\n",
        "Clean the Kaggle Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEGNITxnZ24Y"
      },
      "source": [
        "import json\n",
        "with open('trainTone.clean.txt', 'wt') as writer:\n",
        "  writer.writelines('category,description\\n')\n",
        "  lines = json.load(open('trainTone.txt'))\n",
        "  for line in lines:\n",
        "    tone,sentence = line['tone'],line['sentence']\n",
        "    writer.writelines(f'{tone},\"{sentence}\"\\n')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX0EQzxxaBWY"
      },
      "source": [
        "###Load the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6cn4MqNaEsz",
        "outputId": "75138b42-abe3-4e4b-baa6-66ff6e82d822"
      },
      "source": [
        "trainDataset = spark.read \\\n",
        "    .option(\"header\", True) \\\n",
        "    .csv(\"trainTone.clean.txt\")\n",
        "\n",
        "trainDataset.show(truncate=50, n=5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------------------------------------+\n",
            "|category|                                       description|\n",
            "+--------+--------------------------------------------------+\n",
            "| sadness|                           i didnt feel humiliated|\n",
            "| sadness|i can go from feeling so hopeless to so damned ...|\n",
            "|   anger|  im grabbing a minute to post i feel greedy wrong|\n",
            "|    love|i am ever feeling nostalgic about the fireplace...|\n",
            "|   anger|                              i am feeling grouchy|\n",
            "+--------+--------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXIrEUH9aPK2"
      },
      "source": [
        "###Checkout the category distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujwwvL01RZRn",
        "outputId": "551d1c1e-af87-45d7-f186-74d453e2e112"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "trainDataset.groupBy(\"category\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc()) \\\n",
        "    .show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|category|count|\n",
            "+--------+-----+\n",
            "|     joy| 5362|\n",
            "| sadness| 4666|\n",
            "|   anger| 2159|\n",
            "|    fear| 1937|\n",
            "|    love| 1304|\n",
            "|surprise|  572|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JB4vCseaUV5"
      },
      "source": [
        "##Assemble the model pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EXmCQymR5rB",
        "outputId": "050a725f-f939-4738-930f-43efcb03079b"
      },
      "source": [
        "# actual content is inside description column\n",
        "document = DocumentAssembler()\\\n",
        "      .setInputCol(\"description\")\\\n",
        "      .setOutputCol(\"document\")\n",
        "\n",
        "# we can also use sentece detector here if we want to train on and get predictions for each sentence\n",
        "use = UniversalSentenceEncoder.pretrained(\"tfhub_use_lg\", \"en\") \\\n",
        "      .setInputCols(\"document\") \\\n",
        "      .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "# the classes/labels/categories are in category column\n",
        "classifierdl = ClassifierDLApproach()\\\n",
        "      .setInputCols([\"sentence_embeddings\"])\\\n",
        "      .setOutputCol(\"class\")\\\n",
        "      .setLabelColumn(\"category\")\\\n",
        "      .setMaxEpochs(30)\\\n",
        "      .setBatchSize(8)\\\n",
        "      .setEnableOutputLogs(True)\n",
        "\n",
        "use_clf_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        use,\n",
        "        classifierdl\n",
        "    ])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfhub_use_lg download started this may take some time.\n",
            "Approximate size to download 753.3 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHww7Mw0aakv"
      },
      "source": [
        "##Run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E8rwF0JR9-q",
        "outputId": "f7ae8734-e262-40ff-9528-62f1959111ee"
      },
      "source": [
        "%%time\n",
        "clf_pipelineModel = use_clf_pipeline.fit(trainDataset)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.04 s, sys: 492 ms, total: 5.53 s\n",
            "Wall time: 16min 41s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Ns2vU6afKB"
      },
      "source": [
        "##Check the logs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqVc_SUhUTHp",
        "outputId": "03bbfd5c-bb79-4017-ac48-4cd3a30ac903"
      },
      "source": [
        "import os\n",
        "log_file_name = os.listdir(\"/root/annotator_logs\")[0]\n",
        "\n",
        "with open(\"/root/annotator_logs/\"+log_file_name, \"r\") as log_file :\n",
        "    print(log_file.read())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started - epochs: 30 - learning_rate: 0.005 - batch_size: 8 - training_examples: 16000 - classes: 6\n",
            "Epoch 0/30 - 16.63s - loss: 3014.3157 - acc: 0.524875 - batches: 2000\n",
            "Epoch 1/30 - 15.17s - loss: 2906.148 - acc: 0.5685625 - batches: 2000\n",
            "Epoch 2/30 - 15.16s - loss: 2883.1816 - acc: 0.5834375 - batches: 2000\n",
            "Epoch 3/30 - 14.90s - loss: 2825.0278 - acc: 0.62 - batches: 2000\n",
            "Epoch 4/30 - 15.07s - loss: 2776.0994 - acc: 0.64575 - batches: 2000\n",
            "Epoch 5/30 - 15.09s - loss: 2750.424 - acc: 0.661 - batches: 2000\n",
            "Epoch 6/30 - 15.03s - loss: 2731.1006 - acc: 0.6709375 - batches: 2000\n",
            "Epoch 7/30 - 16.46s - loss: 2716.7332 - acc: 0.6780625 - batches: 2000\n",
            "Epoch 8/30 - 15.09s - loss: 2705.637 - acc: 0.68425 - batches: 2000\n",
            "Epoch 9/30 - 15.40s - loss: 2693.9539 - acc: 0.689375 - batches: 2000\n",
            "Epoch 10/30 - 15.09s - loss: 2682.9326 - acc: 0.6938125 - batches: 2000\n",
            "Epoch 11/30 - 15.30s - loss: 2672.5469 - acc: 0.697375 - batches: 2000\n",
            "Epoch 12/30 - 15.48s - loss: 2663.7043 - acc: 0.7013125 - batches: 2000\n",
            "Epoch 13/30 - 15.20s - loss: 2656.193 - acc: 0.7054375 - batches: 2000\n",
            "Epoch 14/30 - 15.12s - loss: 2648.3013 - acc: 0.7086875 - batches: 2000\n",
            "Epoch 15/30 - 15.28s - loss: 2643.3533 - acc: 0.7123125 - batches: 2000\n",
            "Epoch 16/30 - 15.08s - loss: 2638.942 - acc: 0.7153125 - batches: 2000\n",
            "Epoch 17/30 - 15.06s - loss: 2634.4155 - acc: 0.7178125 - batches: 2000\n",
            "Epoch 18/30 - 15.53s - loss: 2629.4895 - acc: 0.71975 - batches: 2000\n",
            "Epoch 19/30 - 15.72s - loss: 2626.4775 - acc: 0.721375 - batches: 2000\n",
            "Epoch 20/30 - 15.75s - loss: 2621.984 - acc: 0.7229375 - batches: 2000\n",
            "Epoch 21/30 - 15.61s - loss: 2617.9832 - acc: 0.725 - batches: 2000\n",
            "Epoch 22/30 - 15.63s - loss: 2612.3137 - acc: 0.7269375 - batches: 2000\n",
            "Epoch 23/30 - 15.37s - loss: 2607.94 - acc: 0.72875 - batches: 2000\n",
            "Epoch 24/30 - 15.37s - loss: 2604.811 - acc: 0.73 - batches: 2000\n",
            "Epoch 25/30 - 15.34s - loss: 2602.818 - acc: 0.731375 - batches: 2000\n",
            "Epoch 26/30 - 15.51s - loss: 2599.5608 - acc: 0.7325 - batches: 2000\n",
            "Epoch 27/30 - 15.32s - loss: 2597.4465 - acc: 0.7336875 - batches: 2000\n",
            "Epoch 28/30 - 15.26s - loss: 2595.6238 - acc: 0.7345 - batches: 2000\n",
            "Epoch 29/30 - 15.28s - loss: 2592.9998 - acc: 0.7360625 - batches: 2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKiKZYTVamMI"
      },
      "source": [
        "##Examine the predictions of the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz6QhOEuUU4X",
        "outputId": "0b8d6679-18e2-4d15-d9a2-67be97511ecc"
      },
      "source": [
        "preds = clf_pipelineModel.transform(trainDataset)\n",
        "preds.select('category','description',\"class.result\").show(n=5, truncate=50)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------------------------------------+---------+\n",
            "|category|                                       description|   result|\n",
            "+--------+--------------------------------------------------+---------+\n",
            "| sadness|                           i didnt feel humiliated|[sadness]|\n",
            "| sadness|i can go from feeling so hopeless to so damned ...|[sadness]|\n",
            "|   anger|  im grabbing a minute to post i feel greedy wrong|  [anger]|\n",
            "|    love|i am ever feeling nostalgic about the fireplace...|    [joy]|\n",
            "|   anger|                              i am feeling grouchy|  [anger]|\n",
            "+--------+--------------------------------------------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaZ05ng_atDW"
      },
      "source": [
        "##Get model training metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqBatffUVDOE"
      },
      "source": [
        "preds_df = preds.select('category','description',\"class.result\").toPandas()\n",
        "preds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(preds_df['result'], preds_df['category']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzlY9ApFrX4r"
      },
      "source": [
        "##Predict an example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWrYBTojlemD",
        "outputId": "b0d3b379-8626-4e30-dd69-34b71a26e7a2"
      },
      "source": [
        "from sparknlp.base import LightPipeline\n",
        "light_model = LightPipeline(clf_pipelineModel)\n",
        "text = 'i am not happy'\n",
        "light_model.annotate(text)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'class': ['sadness'],\n",
              " 'document': ['i am not happy'],\n",
              " 'sentence_embeddings': ['i am not happy']}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6zO1wCc9dtO"
      },
      "source": [
        "##Save the pipeline\n",
        "###Do the actual saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UON9m8H9wJr"
      },
      "source": [
        "clf_pipelineModel.write().save('ToneItPipeline')\n",
        "!tar czvf ToneItPipeline.tar.gz ToneItPipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkJzTsa5_IoN"
      },
      "source": [
        "###Check on the size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcLBk7ko_AMf",
        "outputId": "85e8ae09-5467-4527-daed-7e90abfa546f"
      },
      "source": [
        "!du -sch ToneItPipeline"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "872M\tToneItPipeline\n",
            "872M\ttotal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEt4oBxjCxKg",
        "outputId": "2eba4534-0ed4-4c0e-bc54-cd3e20558ae7"
      },
      "source": [
        "!du -sch ToneItPipeline.tar.gz"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "775M\tToneItPipeline.tar.gz\n",
            "775M\ttotal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwN-2jGj-jyy"
      },
      "source": [
        "##Load the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_in3IuQAHZ2",
        "outputId": "c282e06c-0234-403f-a664-cdab38f7eef8"
      },
      "source": [
        "import sparknlp\n",
        "sparknlp.start()\n",
        "from pyspark.ml import PipelineModel\n",
        "from sparknlp.base import LightPipeline\n",
        "ToneItPipeline = LightPipeline(PipelineModel.load('ToneItPipeline'))\n",
        "ToneItPipeline.annotate(\"we fell to the floor our faces pale\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'class': ['sadness'],\n",
              " 'document': ['we fell to the floor our faces pale'],\n",
              " 'sentence_embeddings': ['we fell to the floor our faces pale']}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKaVCi5ws2th"
      },
      "source": [
        "universalsentenceencoder --> 0.69<br>\n",
        "tokenizer+bertsmall+sentence --> 0.29!!!ALLSAD<br>\n",
        "bertsmallsent --> 0.35!!!ALLJOY<br>\n",
        "bertusecmlmenbase --> 0.35!!!ALLJOY"
      ]
    }
  ]
}